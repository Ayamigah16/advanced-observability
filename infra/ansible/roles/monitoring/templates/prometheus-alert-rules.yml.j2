groups:
  - name: app_and_host_alerts
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{job="python-app",status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job="python-app"}[5m]))
          ) * 100 > {{ alert_error_rate_threshold }}
        for: 5m
        labels:
          severity: critical
          owner: platform
        annotations:
          summary: "High application error rate (>{{ alert_error_rate_threshold }}%)"
          description: "5xx error percentage has exceeded {{ alert_error_rate_threshold }}% for at least 5 minutes. Current value: {{ "{{" }} $value | humanize {{ "}}" }}%"

      - alert: HighLatencyP95
        expr: |
          histogram_quantile(
            0.95,
            sum(rate(http_request_duration_seconds_bucket{job="python-app"}[5m])) by (le)
          ) > {{ alert_latency_p95_threshold }}
        for: 10m
        labels:
          severity: warning
          owner: platform
        annotations:
          summary: "High application latency (p95 > {{ alert_latency_p95_threshold }}s)"
          description: "p95 latency has remained above {{ alert_latency_p95_threshold }}s for 10 minutes. Current value: {{ "{{" }} $value | humanizeDuration {{ "}}" }}"

      - alert: HighCPUUsage
        expr: |
          (100 - (avg by (instance) (
            rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[5m])
          ) * 100)) > {{ alert_cpu_threshold }}
        for: 10m
        labels:
          severity: warning
          owner: platform
        annotations:
          summary: "High EC2 CPU usage (> {{ alert_cpu_threshold }}%)"
          description: "Host CPU utilization has exceeded {{ alert_cpu_threshold }}% for at least 10 minutes on {{ "{{" }} $labels.instance {{ "}}" }}."

  - name: monitoring_self_health
    rules:
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          owner: platform
        annotations:
          summary: "Prometheus scrape target is DOWN"
          description: "Target {{ "{{" }} $labels.job {{ "}}" }} / {{ "{{" }} $labels.instance {{ "}}" }} has been unreachable for 2 minutes."

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: critical
          owner: platform
        annotations:
          summary: "Alertmanager is DOWN"
          description: "Alertmanager has been unreachable for 2 minutes â€” alert routing is broken."
